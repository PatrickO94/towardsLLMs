# towardsLLMs
A Sandbox for me to learn all about LLMs.
- Used Andrej Karpathy's YT-guide ['Let's build GPT: from scratch, in code, spelled out.'](https://youtu.be/kCc8FmEb1nY) to learn more about attention and create a first model
- Adapted the project to use [OpenWebText2 by Geralt-Targaryen from Huggingface](https://huggingface.co/datasets/Geralt-Targaryen/openwebtext2)

Next Steps:
- Pre-train and optimize a model until it generates good-ish quality text (also get better HW, 16gb VRAM won't cut it)
- Fine-tune (perhaps implementing [LoRA](https://arxiv.org/abs/2106.09685)?) to conversation-style and for generating context dependent action-tokens (for robot)
- How tf to implement [RLHF](https://arxiv.org/abs/2203.02155) from scratch?
